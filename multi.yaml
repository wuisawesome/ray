# EXPERIMENTAL: an example of configuring a mixed-worker cluster. Currently
# multi-worker autoscaling only works if you use the request_resources() call.
cluster_name: alex_test
min_workers: 0
max_workers: 5

# Cloud-provider specific configuration.
provider:
    type: aws
    region: us-west-2
    availability_zone: us-west-2a

# Tell the autoscaler the allowed node types and the resources they provide.
# The key is the name of the node type, which is just for debugging purposes.
# The node config specifies the launch config and physical instance type.
available_node_types:
    cpu_4_ondemand:
        node_config:
            # For more documentation on available fields, see:
            # http://boto3.readthedocs.io/en/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances
            InstanceType: m4.xlarge
        resources: {"CPU": 4}
        max_workers: 5
    cpu_4_spot:
        node_config:
            InstanceType: m4.xlarge
            InstanceMarketOptions:
                MarketType: spot
        resources: {"CPU": 4}
        max_workers: 20
    cpu_16_ondemand:
        node_config:
            InstanceType: m4.4xlarge
        resources: {"CPU": 16, "Custom1": 1}
        max_workers: 10
    gpu_1_ondemand:
        node_config:
            InstanceType: p2.xlarge
        resources: {"CPU": 4, "GPU": 1, "Custom2": 2}
        max_workers: 4
    gpu_8_ondemand:
        node_config:
            InstanceType: p2.8xlarge
        resources: {"CPU": 32, "GPU": 8}
        max_workers: 2

# Specify the node type of the head node (as configured above).
head_node_type: cpu_4_ondemand

# Specify the default type of the worker node (as configured above).
worker_default_node_type: cpu_4_spot

# The default settings for the head node. This will be merged with the per-node
# type configs given above.
head_node:
    ImageId: latest_dlami

# The default settings for worker nodes. This will be merged with the per-node
# type configs given above.
worker_nodes:
    ImageId: latest_dlami

# Configure the cluster for very conservative auto-scaling otherwise.
target_utilization_fraction: 1.0
idle_timeout_minutes: 2

setup_commands:
    - sudo pkill -9 apt-get || true
    - sudo pkill -9 dpkg || true
    - sleep 1 && sudo dpkg --configure -a
    - sudo apt-get update
    - sudo apt-get install -y build-essential curl unzip
    # Install Node.js in order to build the dashboard.
    - curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash
    - sudo apt-get install -y nodejs
    # Install Anaconda.
    - wget https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh || true
    - bash Anaconda3-5.0.1-Linux-x86_64.sh -b -p $HOME/anaconda3 || true
    - echo 'export PATH="$HOME/anaconda3/bin:$PATH"' >> ~/.bashrc
    # Build Ray.
    - git clone --single-branch --branch autoscaler-provider-config2 https://github.com/wuisawesome/ray || true
    - ray/ci/travis/install-bazel.sh
    - cd ray/python/ray/dashboard/client; npm ci; npm run build
    - pip install boto3==1.4.8 cython==0.29.0 aiohttp grpcio psutil setproctitle
    - cd ray/python; pip install -e . --verbose

